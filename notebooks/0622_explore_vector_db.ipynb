{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliev/miniforge3/envs/metrics/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pinecone\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "import components.pinecone_langchain as plc\n",
    "\n",
    "\n",
    "import scrapeandindex.sparsevectors as sv\n",
    "import scrapeandindex.query as query_sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_env = os.getenv('PINECONE_ENV')\n",
    "pinecone.init(api_key=pinecone_key, environment=pinecone_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pinehack']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pinecone.Index('pinehack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.index.Index at 0x162a77950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchaindocs': {'vector_count': 1273}},\n",
       " 'total_vector_count': 1273}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilang = plc.LangChainPineconeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I cannot verify the accuracy of the information provided in the source. However, based on the information provided, the text appears to be up to date and accurate. The source describes LangChain as a framework for developing applications powered by language models, which enables applications that are data-aware and agentic. It also highlights the main value propositions of LangChain, including components and off-the-shelf chains, which make it easy to get started and customize existing chains or build new ones. Finally, the source confirms that LangChain is a tool for categorizing text into different topics. Source: Introduction LangChain is a framework for developing applications powered by language models. It enables applications that are: Data-aware: connect a language model to other sources of data Agentic: allow a language model to interact with its environment The main value props of LangChain are: Components: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the only function needed by the frontend\n",
    "pilang.ask_with_context(\"Langchain is a tool for categorizing text into different topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is the subject of the document.', additional_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.get_relevant_text(\"How do I ask questions over documents using LangChain?\", \"LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.get_relevant_pinecone_data(\"How do I ask questions over documents using LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchaindocs': {'vector_count': 1273}},\n",
       " 'total_vector_count': 1273}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = pilang.embed.embed_query(\"How do I ask questions over documents using LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pilang.index.query(\n",
    "namespace='langchaindocs',\n",
    "top_k=1,\n",
    "vector=embedded_query,\n",
    "sparse_vector = sv.get_sparse_vector(\"How do I ask questions over documents using LangChain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id = result['matches'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '875899727194390529', 'score': 25.1728687, 'values': []}],\n",
       " 'namespace': 'langchaindocs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "DATABASE_URL = os.environ['DATABASE_URL']\n",
    "conn = psycopg2.connect(DATABASE_URL)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = query_sql.get_heading_by_rowid('875899727194390529', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9',\n",
       " 'Document Question Answering\\u200b',\n",
       " '{}',\n",
       " '{1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670}',\n",
       " True,\n",
       " 'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selection\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selection' is not defined"
     ]
    }
   ],
   "source": [
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scrapeandindex.query import select_row_by_index\n",
    "# get the heading in the 5th row of the headings table\n",
    "heading = select_row_by_index(500, 'headings', '*', conn)\n",
    "# print(heading)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('66',\n",
       " 'Databricks',\n",
       " '{502,503,507}',\n",
       " '{5581}',\n",
       " True,\n",
       " 'Databricks This notebook covers how to connect to the Databricks runtimes and Databricks SQL using the SQLDatabase wrapper of LangChain.\\nIt is broken into 3 parts: installation and setup, connecting to Databricks, and examples.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the url at the urlid in the headings table\n",
    "url = select_row_by_index(heading[0], 'urls', '*', conn)\n",
    "# print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://python.langchain.com/docs/ecosystem/integrations/databricks/',\n",
       " True,\n",
       " datetime.datetime(2023, 6, 21, 15, 6, 59, 97449))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://python.langchain.com/docs/use_cases/question_answering/',\n",
       " True,\n",
       " datetime.datetime(2023, 6, 21, 14, 56, 23, 132373))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sql.get_url_by_headingid('875899727194390529', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
