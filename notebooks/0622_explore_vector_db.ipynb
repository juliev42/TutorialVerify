{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliev/miniforge3/envs/metrics/lib/python3.11/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pinecone\n",
    "\n",
    "currentdir = os.getcwd()\n",
    "\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "import components.pinecone_langchain as plc\n",
    "\n",
    "\n",
    "import scrapeandindex.sparsevectors as sv\n",
    "import scrapeandindex.query as query_sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key = os.getenv('PINECONE_API_KEY')\n",
    "pinecone_env = os.getenv('PINECONE_ENV')\n",
    "pinecone.init(api_key=pinecone_key, environment=pinecone_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pinehack']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pinecone.Index('pinehack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.index.Index at 0x162a77950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchaindocs': {'vector_count': 1273}},\n",
       " 'total_vector_count': 1273}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilang = plc.LangChainPineconeClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I cannot verify the accuracy of the information provided in the source. However, based on the information provided, the text appears to be up to date and accurate. The source describes LangChain as a framework for developing applications powered by language models, which enables applications that are data-aware and agentic. It also highlights the main value propositions of LangChain, including components and off-the-shelf chains, which make it easy to get started and customize existing chains or build new ones. Finally, the source confirms that LangChain is a tool for categorizing text into different topics. Source: Introduction LangChain is a framework for developing applications powered by language models. It enables applications that are: Data-aware: connect a language model to other sources of data Agentic: allow a language model to interact with its environment The main value props of LangChain are: Components: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the only function needed by the frontend\n",
    "pilang.ask_with_context(\"Langchain is a tool for categorizing text into different topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is the subject of the document.', additional_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.get_relevant_text(\"How do I ask questions over documents using LangChain?\", \"LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.get_relevant_pinecone_data(\"How do I ask questions over documents using LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchaindocs': {'vector_count': 1273}},\n",
       " 'total_vector_count': 1273}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pilang.index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_query = pilang.embed.embed_query(\"How do I ask questions over documents using LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pilang.index.query(\n",
    "namespace='langchaindocs',\n",
    "top_k=1,\n",
    "vector=embedded_query,\n",
    "sparse_vector = sv.get_sparse_vector(\"How do I ask questions over documents using LangChain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id = result['matches'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '875899727194390529', 'score': 25.1728687, 'values': []}],\n",
       " 'namespace': 'langchaindocs'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "DATABASE_URL = os.environ['DATABASE_URL']\n",
    "conn = psycopg2.connect(DATABASE_URL)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = query_sql.get_heading_by_rowid('875899727194390529', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('9',\n",
       " 'Document Question Answering\\u200b',\n",
       " '{}',\n",
       " '{1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670}',\n",
       " True,\n",
       " 'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document Question Answering\\u200b Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents. The recommended way to get started using a question answering chain is: from langchain . chains . question_answering import load_qa_chain chain = load_qa_chain ( llm , chain_type = \"stuff\" ) chain . run ( input_documents = docs , question = query ) The following resources exist: Question Answering Notebook: A notebook walking through how to accomplish this task. VectorDB Question Answering Notebook: A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_sql\u001b[39m.\u001b[39;49mget_url_by_headingid(\u001b[39m'\u001b[39;49m\u001b[39m9\u001b[39;49m\u001b[39m'\u001b[39;49m, cur)\n",
      "File \u001b[0;32m~/Dropbox/Code/TutorialVerify/scrapeandindex/query.py:31\u001b[0m, in \u001b[0;36mget_url_by_headingid\u001b[0;34m(rowid, cur)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_url_by_headingid\u001b[39m(rowid, cur):\n\u001b[1;32m     30\u001b[0m     heading \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mSELECT urlid FROM headings WHERE rowid = \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, (rowid,))\n\u001b[0;32m---> 31\u001b[0m     url \u001b[39m=\u001b[39m select_row_by_index(heading[\u001b[39m0\u001b[39;49m], \u001b[39m'\u001b[39m\u001b[39murls\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m url\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "query_sql.get_url_by_headingid('9', cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pilang\u001b[39m.\u001b[39;49mask_with_context(\u001b[39m\"\u001b[39;49m\u001b[39mLangchain is a tool for categorizint text into different topics\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Dropbox/Code/TutorialVerify/components/pinecone_langchain.py:106\u001b[0m, in \u001b[0;36mLangChainPineconeClient.ask_with_context\u001b[0;34m(self, input, topic)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mask_with_context\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, topic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLangChain or prompting LLMs with chains of text\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    105\u001b[0m     relevant_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_relevant_text(\u001b[39minput\u001b[39m, topic)\n\u001b[0;32m--> 106\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_relevant_pinecone_data(relevant_text)\n\u001b[1;32m    108\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUsing the following source, verify that the text is up to date and accurate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     total_prompt \u001b[39m=\u001b[39m prompt \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Source: \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Text: \u001b[39m\u001b[39m{\u001b[39;00mrelevant_text\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/Dropbox/Code/TutorialVerify/components/pinecone_langchain.py:88\u001b[0m, in \u001b[0;36mLangChainPineconeClient.get_relevant_pinecone_data\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_relevant_pinecone_data\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m     87\u001b[0m     \u001b[39m##TODO change to check against multiple sources\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     embedded_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed\u001b[39m.\u001b[39;49membed_query(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     89\u001b[0m     query_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mquery(namespace\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlangchaindocs\u001b[39m\u001b[39m'\u001b[39m, top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \\\n\u001b[1;32m     90\u001b[0m                                      vector\u001b[39m=\u001b[39membedded_query, \n\u001b[1;32m     91\u001b[0m                                      sparse_vector \u001b[39m=\u001b[39m sv\u001b[39m.\u001b[39mget_sparse_vector(\u001b[39minput\u001b[39m))\n\u001b[1;32m     92\u001b[0m     match_id \u001b[39m=\u001b[39m query_results[\u001b[39m'\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/metrics/lib/python3.11/site-packages/langchain/embeddings/openai.py:297\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_func(text, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_model_name)\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/miniforge3/envs/metrics/lib/python3.11/site-packages/langchain/embeddings/openai.py:252\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._embedding_func\u001b[0;34m(self, text, engine)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m# handle large input text\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings([text], engine\u001b[39m=\u001b[39;49mengine)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/metrics/lib/python3.11/site-packages/langchain/embeddings/openai.py:209\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    206\u001b[0m encoding \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mencoding_for_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_model_name)\n\u001b[1;32m    207\u001b[0m \u001b[39mfor\u001b[39;00m i, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(texts):\n\u001b[1;32m    208\u001b[0m     \u001b[39m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m     token \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39mencode(\n\u001b[1;32m    211\u001b[0m         text,\n\u001b[1;32m    212\u001b[0m         allowed_special\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallowed_special,\n\u001b[1;32m    213\u001b[0m         disallowed_special\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisallowed_special,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(token), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_ctx_length):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AIMessage' object has no attribute 'replace'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
